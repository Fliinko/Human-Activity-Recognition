{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up the Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import statistics as st\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as colors\n",
    "\n",
    "from scipy import stats\n",
    "from datetime import datetime\n",
    "from statsmodels import robust\n",
    "from scipy.fft import fft"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Our Raw Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correlation(data1, data2):\n",
    "    corr, p = stats.pearsonr(data1, data2)\n",
    "    if math.isnan(corr):\n",
    "        return 0\n",
    "    else:\n",
    "        return corr\n",
    "\n",
    "def sma(x, y, z):\n",
    "    sum_ = 0\n",
    "    X = list(x)\n",
    "    Y = list(y)\n",
    "    Z = list(z)\n",
    "    for i in range(len(X)):\n",
    "        sum_ += abs(X[i]) + abs(Y[i]) + abs(Z[i])\n",
    "    return sum_ / len(X)\n",
    "\n",
    "def calc_entropy(data):\n",
    "    entropy = stats.entropy(data, base=2)\n",
    "    if math.isinf(entropy) or math.isnan(entropy):\n",
    "        return -1\n",
    "    else:\n",
    "        return entropy\n",
    "\n",
    "def energy(data):\n",
    "    sum_ = 0\n",
    "    for d in data:\n",
    "        sum_ += d ** 2\n",
    "        \n",
    "    return sum_ / len(data)\n",
    "\n",
    "def iqr(data):\n",
    "    return np.subtract(*np.percentile(data, [75, 25]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    -0.8857\n",
      "1    -0.8139\n",
      "2   -10.2466\n",
      "3    -4.3679\n",
      "4     5.0936\n",
      "Name: ACCELEROMETER_X_, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "raw_datasets = []\n",
    "raw_datalabels = []\n",
    "\n",
    "directory = \"RawDataSet\"\n",
    "\n",
    "for filename in os.listdir(directory):\n",
    "    if filename.endswith(\".csv\"):\n",
    "        raw = pd.read_csv(os.path.join(directory, filename))\n",
    "        cols = raw.columns\n",
    "        cols = cols.str.replace('([\\(\\[]).*?([\\)\\]])', '')\n",
    "        cols = cols.str.replace('\\s','_')\n",
    "        raw.columns = cols\n",
    "        to_drop = []\n",
    "        for col in raw.columns:\n",
    "            if not (\"_X_\" in col or \"_Y_\" in col or \"_Z_\" in col):\n",
    "                to_drop.append(col)\n",
    "        raw = raw.drop(to_drop, axis=1)\n",
    "        column_names = raw.columns\n",
    "        raw_datasets.append(raw)\n",
    "        raw_datalabels.append(filename.split(\".\")[0])\n",
    "\n",
    "raw = raw_datasets[0][\"ACCELEROMETER_X_\"][:5]\n",
    "\n",
    "print(raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "datasets = dict()\n",
    "statistics = [\"mean\", \"mad\", \"max\", \"min\", \"std\", \"energy\", \"iqr\", \"entropy\"]\n",
    "diff_col_names = []\n",
    "for col in range(0,len(column_names),3):\n",
    "    diff_col_names.append(column_names[col][:len(column_names[col])-3])\n",
    "    \n",
    "for col in column_names:\n",
    "    for stat in statistics:\n",
    "        key = col + \"~\" + stat\n",
    "        datasets[key] = []\n",
    "        \n",
    "for col in diff_col_names:\n",
    "    datasets[col+\"_XY_~correlation\"] = []\n",
    "    datasets[col+\"_YZ_~correlation\"] = []\n",
    "    datasets[col+\"_ZX_~correlation\"] = []\n",
    "    datasets[col+\"_XYZ_~sma\"] = []\n",
    "    \n",
    "datasets[\"Activity\"] = []\n",
    "\n",
    "for ind,raw_data in enumerate(raw_datasets):\n",
    "    print(ind)\n",
    "    for d in range(0, len(raw_data), 5):\n",
    "        if d+5 < len(raw_data):\n",
    "            data = raw_data[d:d+5]\n",
    "        else:\n",
    "            break\n",
    "        for c in diff_col_names:\n",
    "            col_X = c + \"_X_\"\n",
    "            col_Y = c + \"_Y_\"\n",
    "            col_Z = c + \"_Z_\"\n",
    "\n",
    "            datasets[col_X+\"~mean\"].append(st.mean(data[col_X])) # mean X\n",
    "            datasets[col_Y+\"~mean\"].append(st.mean(data[col_Y])) # mean Y\n",
    "            datasets[col_Z+\"~mean\"].append(st.mean(data[col_Z])) # mean Z\n",
    "\n",
    "            datasets[col_X+\"~mad\"].append(robust.mad(np.array(data[col_X]))) # median absolute deviation X\n",
    "            datasets[col_Y+\"~mad\"].append(robust.mad(np.array(data[col_Y]))) # median absolute deviation Y\n",
    "            datasets[col_Z+\"~mad\"].append(robust.mad(np.array(data[col_Z]))) # median absolute deviation Z\n",
    "\n",
    "            datasets[col_X+\"~max\"].append(max(data[col_X])) # maximum X\n",
    "            datasets[col_Y+\"~max\"].append(max(data[col_Y])) # maximum Y\n",
    "            datasets[col_Z+\"~max\"].append(max(data[col_Z])) # maximum Z\n",
    "\n",
    "            datasets[col_X+\"~min\"].append(min(data[col_X])) # minimum X\n",
    "            datasets[col_Y+\"~min\"].append(min(data[col_Y])) # minimum Y\n",
    "            datasets[col_Z+\"~min\"].append(min(data[col_Z])) # minimum Z\n",
    "\n",
    "            datasets[col_X+\"~std\"].append(st.stdev(data[col_X])) # standard deviation X\n",
    "            datasets[col_Y+\"~std\"].append(st.stdev(data[col_Y])) # standard deviation Y\n",
    "            datasets[col_Z+\"~std\"].append(st.stdev(data[col_Z])) # standard deviation Z\n",
    "\n",
    "            datasets[col_X+\"~energy\"].append(energy(data[col_X])) # energy X\n",
    "            datasets[col_Y+\"~energy\"].append(energy(data[col_Y])) # energy Y\n",
    "            datasets[col_Z+\"~energy\"].append(energy(data[col_Z])) # energy Z\n",
    "\n",
    "            datasets[col_X+\"~iqr\"].append(iqr(data[col_X])) # interquartile range X\n",
    "            datasets[col_Y+\"~iqr\"].append(iqr(data[col_Y])) # interquartile range Y\n",
    "            datasets[col_Z+\"~iqr\"].append(iqr(data[col_Z])) # interquartile range Z\n",
    "            \n",
    "            datasets[col_X+\"~entropy\"].append(calc_entropy(data[col_X])) # entropy X\n",
    "            datasets[col_Y+\"~entropy\"].append(calc_entropy(data[col_Y])) # entropy Y\n",
    "            datasets[col_Z+\"~entropy\"].append(calc_entropy(data[col_Z])) # entropy Z\n",
    "\n",
    "            datasets[c+\"_XY_~correlation\"].append(correlation(data[col_X], data[col_Y])) # correlation between X and Y\n",
    "            datasets[c+\"_YZ_~correlation\"].append(correlation(data[col_Y], data[col_Z])) # correlation between Y and Z\n",
    "            datasets[c+\"_ZX_~correlation\"].append(correlation(data[col_Z], data[col_X])) # correlation between Z and X\n",
    "\n",
    "\n",
    "            datasets[c+\"_XYZ_~sma\"].append(sma(data[col_X], data[col_Y], data[col_Z]))\n",
    "\n",
    "        datasets[\"Activity\"].append(raw_datalabels[ind])\n",
    "        data = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame.from_dict(datasets, orient=\"columns\")\n",
    "df.to_csv(\"Processed_DataSet/ProcessedData.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "can't convert type 'complex128' to numerator/denominator",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-0fd1adcc1a2e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     94\u001b[0m             \u001b[0mfdata_Z\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfft\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcol_Z\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     95\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 96\u001b[1;33m             \u001b[0mdatasets\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mfcol_X\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m\"~mean\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mst\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfdata_X\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# mean X\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     97\u001b[0m             \u001b[0mdatasets\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mfcol_Y\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m\"~mean\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mst\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfdata_Y\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# mean Y\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     98\u001b[0m             \u001b[0mdatasets\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mfcol_Z\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m\"~mean\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mst\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfdata_Z\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# mean Z\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\seanf\\appdata\\local\\programs\\python\\python39\\lib\\statistics.py\u001b[0m in \u001b[0;36mmean\u001b[1;34m(data)\u001b[0m\n\u001b[0;32m    314\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mn\u001b[0m \u001b[1;33m<\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    315\u001b[0m         \u001b[1;32mraise\u001b[0m \u001b[0mStatisticsError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'mean requires at least one data point'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 316\u001b[1;33m     \u001b[0mT\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtotal\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcount\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_sum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    317\u001b[0m     \u001b[1;32massert\u001b[0m \u001b[0mcount\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    318\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0m_convert\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtotal\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mT\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\seanf\\appdata\\local\\programs\\python\\python39\\lib\\statistics.py\u001b[0m in \u001b[0;36m_sum\u001b[1;34m(data, start)\u001b[0m\n\u001b[0;32m    164\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mtyp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalues\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mgroupby\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    165\u001b[0m         \u001b[0mT\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_coerce\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtyp\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# or raise TypeError\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 166\u001b[1;33m         \u001b[1;32mfor\u001b[0m \u001b[0mn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0md\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_exact_ratio\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    167\u001b[0m             \u001b[0mcount\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    168\u001b[0m             \u001b[0mpartials\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0md\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpartials_get\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0md\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\seanf\\appdata\\local\\programs\\python\\python39\\lib\\statistics.py\u001b[0m in \u001b[0;36m_exact_ratio\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m    246\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    247\u001b[0m     \u001b[0mmsg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"can't convert type '{}' to numerator/denominator\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 248\u001b[1;33m     \u001b[1;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    249\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    250\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: can't convert type 'complex128' to numerator/denominator"
     ]
    }
   ],
   "source": [
    "datasets = dict()\n",
    "statistics_t = [\"mean\", \"mad\", \"max\", \"min\", \"std\", \"energy\", \"iqr\", \"entropy\"]\n",
    "statistics_f = statistics_t + [\"skewness\", \"kurtosis\"]\n",
    "diff_col_names = []\n",
    "for col in range(0,len(column_names),3):\n",
    "    diff_col_names.append(column_names[col][:len(column_names[col])-3])\n",
    "    \n",
    "for col in column_names:\n",
    "    for stat in statistics_t:\n",
    "        key = \"t\" + col + \"~\" + stat\n",
    "        datasets[key] = []\n",
    "        \n",
    "for col in diff_col_names:\n",
    "    datasets[\"t\"+col+\"_XY_~correlation\"] = []\n",
    "    datasets[\"t\"+col+\"_YZ_~correlation\"] = []\n",
    "    datasets[\"t\"+col+\"_ZX_~correlation\"] = []\n",
    "    datasets[\"t\"+col+\"_XYZ_~sma\"] = []\n",
    "\n",
    "for col in column_names:\n",
    "    for stat in statistics_f:\n",
    "        key = \"f\" + col + \"~\" + stat\n",
    "        datasets[key] = []\n",
    "\n",
    "for col in diff_col_names:\n",
    "    datasets[\"f\"+col+\"_XYZ_~sma\"] = []\n",
    "    \n",
    "datasets[\"Activity\"] = []\n",
    "\n",
    "for ind,raw_data in enumerate(raw_datasets):\n",
    "    print(ind)\n",
    "    for d in range(0, len(raw_data), 5):\n",
    "        if d+5 < len(raw_data):\n",
    "            data = raw_data[d:d+5]\n",
    "        else:\n",
    "            break\n",
    "        for c in diff_col_names:\n",
    "            col_X = c + \"_X_\"\n",
    "            col_Y = c + \"_Y_\"\n",
    "            col_Z = c + \"_Z_\"\n",
    "            \n",
    "            # time\n",
    "            \n",
    "            tcol_X = \"t\" + col_X\n",
    "            tcol_Y = \"t\" + col_Y\n",
    "            tcol_Z = \"t\" + col_Z\n",
    "\n",
    "            datasets[tcol_X+\"~mean\"].append(st.mean(data[col_X])) # mean X\n",
    "            datasets[tcol_Y+\"~mean\"].append(st.mean(data[col_Y])) # mean Y\n",
    "            datasets[tcol_Z+\"~mean\"].append(st.mean(data[col_Z])) # mean Z\n",
    "\n",
    "            datasets[tcol_X+\"~mad\"].append(robust.mad(np.array(data[col_X]))) # median absolute deviation X\n",
    "            datasets[tcol_Y+\"~mad\"].append(robust.mad(np.array(data[col_Y]))) # median absolute deviation Y\n",
    "            datasets[tcol_Z+\"~mad\"].append(robust.mad(np.array(data[col_Z]))) # median absolute deviation Z\n",
    "\n",
    "            datasets[tcol_X+\"~max\"].append(max(data[col_X])) # maximum X\n",
    "            datasets[tcol_Y+\"~max\"].append(max(data[col_Y])) # maximum Y\n",
    "            datasets[tcol_Z+\"~max\"].append(max(data[col_Z])) # maximum Z\n",
    "\n",
    "            datasets[tcol_X+\"~min\"].append(min(data[col_X])) # minimum X\n",
    "            datasets[tcol_Y+\"~min\"].append(min(data[col_Y])) # minimum Y\n",
    "            datasets[tcol_Z+\"~min\"].append(min(data[col_Z])) # minimum Z\n",
    "\n",
    "            datasets[tcol_X+\"~std\"].append(st.stdev(data[col_X])) # standard deviation X\n",
    "            datasets[tcol_Y+\"~std\"].append(st.stdev(data[col_Y])) # standard deviation Y\n",
    "            datasets[tcol_Z+\"~std\"].append(st.stdev(data[col_Z])) # standard deviation Z\n",
    "\n",
    "            datasets[tcol_X+\"~energy\"].append(energy(data[col_X])) # energy X\n",
    "            datasets[tcol_Y+\"~energy\"].append(energy(data[col_Y])) # energy Y\n",
    "            datasets[tcol_Z+\"~energy\"].append(energy(data[col_Z])) # energy Z\n",
    "\n",
    "            datasets[tcol_X+\"~iqr\"].append(iqr(data[col_X])) # interquartile range X\n",
    "            datasets[tcol_Y+\"~iqr\"].append(iqr(data[col_Y])) # interquartile range Y\n",
    "            datasets[tcol_Z+\"~iqr\"].append(iqr(data[col_Z])) # interquartile range Z\n",
    "            \n",
    "            datasets[tcol_X+\"~entropy\"].append(calc_entropy(data[col_X])) # entropy X\n",
    "            datasets[tcol_Y+\"~entropy\"].append(calc_entropy(data[col_Y])) # entropy Y\n",
    "            datasets[tcol_Z+\"~entropy\"].append(calc_entropy(data[col_Z])) # entropy Z\n",
    "\n",
    "            datasets[\"t\"+c+\"_XY_~correlation\"].append(correlation(data[col_X], data[col_Y])) # correlation between X and Y\n",
    "            datasets[\"t\"+c+\"_YZ_~correlation\"].append(correlation(data[col_Y], data[col_Z])) # correlation between Y and Z\n",
    "            datasets[\"t\"+c+\"_ZX_~correlation\"].append(correlation(data[col_Z], data[col_X])) # correlation between Z and X\n",
    "\n",
    "\n",
    "            datasets[\"t\"+c+\"_XYZ_~sma\"].append(sma(data[col_X], data[col_Y], data[col_Z]))\n",
    "            \n",
    "            # frequency\n",
    "            \n",
    "            fcol_X = \"f\" + col_X\n",
    "            fcol_Y = \"f\" + col_Y\n",
    "            fcol_Z = \"f\" + col_Z\n",
    "            \n",
    "            fdata_X = fft(list(data[col_X]))\n",
    "            fdata_Y = fft(list(data[col_Y]))\n",
    "            fdata_Z = fft(list(data[col_Z]))\n",
    "            \n",
    "            datasets[fcol_X+\"~mean\"].append(st.mean(fdata_X)) # mean X\n",
    "            datasets[fcol_Y+\"~mean\"].append(st.mean(fdata_Y)) # mean Y\n",
    "            datasets[fcol_Z+\"~mean\"].append(st.mean(fdata_Z)) # mean Z\n",
    "\n",
    "            datasets[fcol_X+\"~mad\"].append(robust.mad(np.array(fdata_X))) # median absolute deviation X\n",
    "            datasets[fcol_Y+\"~mad\"].append(robust.mad(np.array(fdata_Y))) # median absolute deviation Y\n",
    "            datasets[fcol_Z+\"~mad\"].append(robust.mad(np.array(fdata_Z))) # median absolute deviation Z\n",
    "\n",
    "            datasets[fcol_X+\"~max\"].append(max(fdata_X)) # maximum X\n",
    "            datasets[fcol_Y+\"~max\"].append(max(fdata_Y)) # maximum Y\n",
    "            datasets[fcol_Z+\"~max\"].append(max(fdata_Z)) # maximum Z\n",
    "\n",
    "            datasets[fcol_X+\"~min\"].append(min(fdata_X)) # minimum X\n",
    "            datasets[fcol_Y+\"~min\"].append(min(fdata_Y)) # minimum Y\n",
    "            datasets[fcol_Z+\"~min\"].append(min(fdata_Z)) # minimum Z\n",
    "\n",
    "            datasets[fcol_X+\"~std\"].append(st.stdev(fdata_X)) # standard deviation X\n",
    "            datasets[fcol_Y+\"~std\"].append(st.stdev(fdata_Y)) # standard deviation Y\n",
    "            datasets[fcol_Z+\"~std\"].append(st.stdev(fdata_Z)) # standard deviation Z\n",
    "\n",
    "            datasets[fcol_X+\"~energy\"].append(energy(fdata_X)) # energy X\n",
    "            datasets[fcol_Y+\"~energy\"].append(energy(fdata_Y)) # energy Y\n",
    "            datasets[fcol_Z+\"~energy\"].append(energy(fdata_Z)) # energy Z\n",
    "\n",
    "            datasets[fcol_X+\"~iqr\"].append(iqr(fdata_X)) # interquartile range X\n",
    "            datasets[fcol_Y+\"~iqr\"].append(iqr(fdata_Y)) # interquartile range Y\n",
    "            datasets[fcol_Z+\"~iqr\"].append(iqr(fdata_Z)) # interquartile range Z\n",
    "            \n",
    "            datasets[fcol_X+\"~entropy\"].append(calc_entropy(fdata_X)) # entropy X\n",
    "            datasets[fcol_Y+\"~entropy\"].append(calc_entropy(fdata_Y)) # entropy Y\n",
    "            datasets[fcol_Z+\"~entropy\"].append(calc_entropy(fdata_Z)) # entropy Z\n",
    "\n",
    "            datasets[\"f\"+c+\"_XYZ_~sma\"].append(sma(fdata_X, fdata_Y, fdata_Z))\n",
    "            \n",
    "            datasets[fcol_X+\"~kurtosis\"].append(stats.kurtosis(fdata_X)) # kurtosis X\n",
    "            datasets[fcol_Y+\"~kurtosis\"].append(stats.kurtosis(fdata_Y)) # kurtosis Y\n",
    "            datasets[fcol_Z+\"~kurtosis\"].append(stats.kurtosis(fdata_Z)) # kurtosis Z\n",
    "\n",
    "            datasets[fcol_X+\"~skewness\"].append(stats.skew(fdata_X)) # skewness X\n",
    "            datasets[fcol_Y+\"~skewness\"].append(stats.skew(fdata_Y)) # skewness Y\n",
    "            datasets[fcol_Z+\"~skewness\"].append(stats.skew(fdata_Z)) # skewness Z\n",
    "            \n",
    "\n",
    "        datasets[\"Activity\"].append(raw_datalabels[ind])\n",
    "        data = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame.from_dict(datasets, orient=\"columns\")\n",
    "df.to_csv(\"Processed_DataSet/fProcessedData.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
